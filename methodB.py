#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–¹æ³• Bï¼šå¼€å¯ dropout çš„å¤šæ ·æœ¬ç”Ÿæˆ + æ•è·å¹¶ä¿å­˜ DiT è¾“å…¥/è¾“å‡º latentï¼ˆDiffusers / Wan2.1 T2V-1.3Bï¼‰

æ¯ä¸ªæ ·æœ¬ï¼ˆsampleKï¼‰ä¼šå•ç‹¬ç”Ÿæˆç›®å½•ï¼ŒåŒ…å«ï¼š
  sampleK/
    â”œâ”€â”€ output.mp4
    â”œâ”€â”€ dit_inputs.pt          # list[tuple(...)]ï¼Œæ‰€æœ‰å¼ é‡å·² cpu().detach()
    â”œâ”€â”€ dit_outputs.pt         # list[tensor]ï¼ˆæˆ–ä»£è¡¨æ€§ç¬¬ 0 ä¸ª tensorï¼‰ï¼Œå·² cpu().detach()
    â”œâ”€â”€ metadata.json
    â””â”€â”€ step_map.json
  ï¼ˆå¯é€‰ï¼‰dit_inputs_kwargs.json  # forward kwargs çš„ç»“æ„/å¼ é‡å½¢çŠ¶æ‘˜è¦ï¼ˆä¸å«å¤§å¼ é‡ï¼‰

å…³é”®ç‚¹ï¼š
- å¼ºåˆ¶å¼€å¯ DiT çš„ dropoutï¼špipe.transformer.train()
- å¯é€‰ä¹Ÿå¼€å¯æ–‡æœ¬ç¼–ç å™¨çš„ dropoutï¼š--text_dropout
- --same_noiseï¼šæ‰€æœ‰æ ·æœ¬å…±äº«åŒä¸€åˆå§‹å™ªå£°ï¼›å·®å¼‚æ¥æºäº dropout mask
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path

os.environ.setdefault("XFORMERS_DISABLED", "1")  # é¿å… xformers/flash-attn å¯¼å…¥å†²çª

def parse_args():
    p = argparse.ArgumentParser(description="Wan2.1 dropout multi-sample with DiT latent capture")
    # æ¨¡å‹
    p.add_argument("--model_id", default="Wan-AI/Wan2.1-T2V-1.3B-Diffusers")
    p.add_argument("--local_dir", default="./Wan2.1-T2V-1.3B-Diffusers")
    # å­˜å‚¨
    p.add_argument("--save_root", default="./wan_dropout_runs")
    # è§†é¢‘è®¾ç½®
    p.add_argument("--width", type=int, default=832)
    p.add_argument("--height", type=int, default=480)
    p.add_argument("--num_frames", type=int, default=81)
    p.add_argument("--fps", type=int, default=15)
    # é‡‡æ ·ä¸ CFG
    p.add_argument("--guidance_scale", type=float, default=5.5)
    p.add_argument("--steps", type=int, default=30)
    # æç¤ºè¯
    p.add_argument("--prompt", default="A cat walks on the grass, realistic")
    p.add_argument("--negative_prompt", default=("Bright tones, overexposed, static, blurred details, subtitles, "
                                                 "style, works, paintings, images, static, overall gray, worst quality, "
                                                 "low quality, JPEG compression residue, ugly, incomplete, extra fingers, "
                                                 "poorly drawn hands, poorly drawn faces, deformed, disfigured, "
                                                 "misshapen limbs, fused fingers, still picture, messy background, "
                                                 "three legs, many people in the background, walking backwards"))
    # å¤šæ ·æœ¬ & éšæœºæ€§
    p.add_argument("--n_samples", type=int, default=3, help="è¦ç”Ÿæˆçš„æ ·æœ¬æ•°é‡")
    p.add_argument("--base_seed", type=int, default=1234, help="åŸºå‡†ç§å­")
    p.add_argument("--same_noise", action="store_true",
                   help="è®©æ‰€æœ‰æ ·æœ¬ä½¿ç”¨ç›¸åŒåˆå§‹å™ªå£°ï¼ˆåŒºåˆ«ä»…æ¥è‡ª dropoutï¼‰")
    # è®¾å¤‡/å†…å­˜
    p.add_argument("--device", default="cuda", choices=["cuda", "cpu"])
    p.add_argument("--offload", action="store_true", help="å¯ç”¨ CPU-Offload ä»¥çœæ˜¾å­˜")
    p.add_argument("--t5_cpu", action="store_true", help="æŠŠ T5 ç¼–ç å™¨æ”¾åˆ° CPU")
    # dropout æ§åˆ¶
    p.add_argument("--text_dropout", action="store_true", help="åŒæ—¶å¼€å¯æ–‡æœ¬ç¼–ç å™¨çš„ dropoutï¼ˆå¯å¸¦æ¥æ›´å¤šéšæœºæ€§ï¼‰")
    # å¯é€‰ï¼šä¿å­˜ kwargs æ‘˜è¦
    p.add_argument("--save_kwargs_summary", action="store_true",
                   help="ä¿å­˜æ¯æ¬¡ forward çš„ kwargs ç»“æ„æ‘˜è¦åˆ° dit_inputs_kwargs.jsonï¼ˆä¸å«å¤§å¼ é‡ï¼‰")
    return p.parse_args()

def ensure_model(local_dir_str, repo_id):
    local_dir = Path(local_dir_str).expanduser().resolve()
    if not local_dir.exists() or not any(local_dir.iterdir()):
        print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½æ¨¡å‹åˆ°ï¼š{local_dir}")
        from huggingface_hub import snapshot_download
        snapshot_download(repo_id=repo_id, local_dir=str(local_dir), local_dir_use_symlinks=False)
    else:
        print(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç›®å½•ï¼š{local_dir}")
    return local_dir

def to_cpu_detached(obj):
    import torch
    if isinstance(obj, torch.Tensor):
        return obj.detach().to("cpu")
    elif isinstance(obj, (list, tuple)):
        return type(obj)(to_cpu_detached(x) for x in obj)
    elif isinstance(obj, dict):
        return {k: to_cpu_detached(v) for k, v in obj.items()}
    else:
        return obj

def summarize_shapes(obj):
    import torch
    if isinstance(obj, torch.Tensor):
        return {"_tensor": True, "shape": tuple(obj.shape), "dtype": str(obj.dtype)}
    elif isinstance(obj, (list, tuple)):
        return [summarize_shapes(x) for x in obj]
    elif isinstance(obj, dict):
        return {k: summarize_shapes(v) for k, v in obj.items()}
    else:
        if isinstance(obj, (int, float, str, bool)) or obj is None:
            return obj
        return {"_type": type(obj).__name__}

def main():
    args = parse_args()
    import torch
    from diffusers import AutoencoderKLWan, WanPipeline
    from diffusers.utils import export_to_video

    # è®¾å¤‡ & dtype
    if args.device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œåˆ‡æ¢åˆ° CPUï¼ˆè¾ƒæ…¢ï¼‰")
        args.device = "cpu"

    if args.device == "cuda":
        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    else:
        dtype = torch.float32

    # å°ºå¯¸æ ¡éªŒ
    if args.width % 32 != 0 or args.height % 32 != 0:
        raise ValueError("width/height å¿…é¡»æ˜¯ 32 çš„å€æ•°ï¼ˆä¾‹å¦‚ 832x480ï¼‰")

    # æ¨¡å‹
    local_dir = ensure_model(args.local_dir, args.model_id)
    print("ğŸš€ æ­£åœ¨åŠ è½½ç®¡çº¿ â€¦")
    vae = AutoencoderKLWan.from_pretrained(str(local_dir), subfolder="vae",
                                           torch_dtype=(torch.float32 if args.device == "cpu" else torch.float16))
    pipe = WanPipeline.from_pretrained(str(local_dir), vae=vae, torch_dtype=dtype).to(args.device)

    if args.offload:
        try: pipe.enable_sequential_cpu_offload()
        except Exception: pass
        try: pipe.enable_model_cpu_offload()
        except Exception: pass
    if args.t5_cpu:
        try: pipe.text_encoder.to("cpu")
        except Exception: pass

    # å¼€å¯ dropout â€”â€” å…³é”®æ­¥éª¤
    # DiTï¼ˆtransformerï¼‰å¯ç”¨ train() ä»¥æ¿€æ´»å…¶ä¸­çš„ Dropout
    pipe.transformer.train()
    # å¯é€‰ï¼šæ–‡æœ¬ç¼–ç å™¨ä¹Ÿå¼€å¯ dropoutï¼ˆéšæœºæ€§æ›´å¼ºï¼‰
    if args.text_dropout and hasattr(pipe, "text_encoder"):
        try:
            pipe.text_encoder.train()
        except Exception:
            pass

    # æ ¹ç›®å½•
    save_root = Path(args.save_root).expanduser().resolve()
    save_root.mkdir(parents=True, exist_ok=True)
    tag = time.strftime("%Y%m%d-%H%M%S")
    session_dir = save_root / f"session-{tag}"
    session_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆ n ä¸ªæ ·æœ¬
    gen_device = "cuda" if args.device == "cuda" else "cpu"

    # å¦‚æœ same_noise=Trueï¼Œæˆ‘ä»¬ç»™æ‰€æœ‰æ ·æœ¬ç”¨åŒä¸€ä¸ª generatorï¼ˆç›¸åŒåˆå§‹å™ªå£°ï¼‰
    fixed_noise_gen = None
    if args.same_noise:
        fixed_noise_gen = torch.Generator(device=gen_device).manual_seed(int(args.base_seed))

    for k in range(args.n_samples):
        print(f"\nğŸ¬ ç”Ÿæˆæ ·æœ¬ #{k} å¹¶æ•è· latent â€¦")
        sample_dir = session_dir / f"sample{k:02d}"
        sample_dir.mkdir(parents=True, exist_ok=True)

        # æ„å»º generatorï¼š
        # - same_noise=Trueï¼šæ‰€æœ‰æ ·æœ¬å…±äº« fixed_noise_genï¼ˆåŒä¸€åˆå§‹å™ªå£°ï¼‰
        # - same_noise=Falseï¼šæ¯ä¸ªæ ·æœ¬éƒ½æœ‰ä¸åŒåˆå§‹å™ªå£°ï¼ˆbase_seed + kï¼‰
        if args.same_noise:
            generator = fixed_noise_gen
        else:
            generator = torch.Generator(device=gen_device).manual_seed(int(args.base_seed) + k)

        # ä¸ºäº†è®© dropout æ©ç åœ¨æ¯ä¸ªæ ·æœ¬é—´ä¸åŒï¼Œå³ä½¿ same_noise=Trueï¼Œ
        # æˆ‘ä»¬ä¹Ÿæ˜¾å¼æ‰°åŠ¨ PyTorch çš„å…¨å±€ RNGï¼ˆå½±å“ dropoutï¼‰
        torch.manual_seed(int(args.base_seed) + 100000 + k)
        if args.device == "cuda":
            torch.cuda.manual_seed(int(args.base_seed) + 200000 + k)

        # æ•è·å®¹å™¨
        dit_inputs_args = []
        dit_outputs = []
        dit_inputs_kwargs_summary = []

        # hooks
        def pre_hook(mod, inputs, kwargs=None):
            dit_inputs_args.append(tuple(to_cpu_detached(inputs)))
            if args.save_kwargs_summary:
                dit_inputs_kwargs_summary.append(summarize_shapes(kwargs or {}))

        def fwd_hook(mod, inputs, output):
            if isinstance(output, (list, tuple)):
                out = output[0]
            else:
                out = output
            if isinstance(out, torch.Tensor):
                dit_outputs.append(out.detach().to("cpu"))
            else:
                dit_outputs.append(torch.tensor(0))

        # æ³¨å†Œ hooksï¼ˆä¼˜å…ˆ with_kwargsï¼‰
        try:
            pre_h = pipe.transformer.register_forward_pre_hook(pre_hook, with_kwargs=True)
        except TypeError:
            def pre_hook_no_kwargs(mod, inputs):
                pre_hook(mod, inputs, kwargs=None)
            pre_h = pipe.transformer.register_forward_pre_hook(pre_hook_no_kwargs)
        fwd_h = pipe.transformer.register_forward_hook(fwd_hook, with_kwargs=False)

        try:
            # é‡è¦ï¼šåœ¨å¼€å¯ train() ä½†ä»ä¿æŒæ¨ç†å›¾ä¸è¿½è¸ªæ¢¯åº¦
            with torch.no_grad():
                result = pipe(
                    prompt=args.prompt,
                    negative_prompt=args.negative_prompt,
                    height=args.height,
                    width=args.width,
                    num_frames=args.num_frames,
                    guidance_scale=args.guidance_scale,
                    num_inference_steps=args.steps,
                    generator=generator,
                )
        finally:
            try: pre_h.remove()
            except Exception: pass
            try: fwd_h.remove()
            except Exception: pass

        # å¯¼å‡ºè§†é¢‘
        from diffusers.utils import export_to_video
        frames = result.frames[0]
        out_video = sample_dir / "output.mp4"
        export_to_video(frames, str(out_video), fps=args.fps)
        print(f"âœ… å·²ä¿å­˜è§†é¢‘ï¼š{out_video}")

        # ä¿å­˜ latent
        import torch as _torch
        _torch.save(dit_inputs_args, sample_dir / "dit_inputs.pt")
        _torch.save(dit_outputs,    sample_dir / "dit_outputs.pt")
        if args.save_kwargs_summary:
            with open(sample_dir / "dit_inputs_kwargs.json", "w", encoding="utf-8") as f:
                json.dump(dit_inputs_kwargs_summary, f, ensure_ascii=False, indent=2)

        # å…ƒä¿¡æ¯
        meta = {
            "sample_index": k,
            "width": args.width, "height": args.height, "num_frames": args.num_frames, "fps": args.fps,
            "guidance_scale": args.guidance_scale, "num_inference_steps": args.steps,
            "dtype": str(dtype), "device": args.device,
            "model_dir": str(local_dir), "save_dir": str(sample_dir),
            "base_seed": int(args.base_seed),
            "same_noise": bool(args.same_noise),
            "dropout": True,
            "text_dropout": bool(args.text_dropout),
        }
        with open(sample_dir / "metadata.json", "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)

        # å¯å‘å¼ step mapï¼ˆCFG æ—¶æ¯æ­¥ 2 æ¬¡è°ƒç”¨ï¼‰
        approx_calls_per_step = 2 if args.guidance_scale and args.guidance_scale > 1.0 else 1
        call_map = []
        for i in range(len(dit_inputs_args)):
            step_idx = i // approx_calls_per_step
            is_cfg_negative = (i % approx_calls_per_step == 0 and approx_calls_per_step == 2)
            call_map.append({"call_idx": i, "approx_step": int(step_idx), "is_cfg_negative": int(is_cfg_negative)})
        with open(sample_dir / "step_map.json", "w", encoding="utf-8") as f:
            json.dump({"calls": call_map}, f, ensure_ascii=False, indent=2)

        print(f"ğŸ” æ•è·æ£€æŸ¥ï¼šinputs={len(dit_inputs_args)}, outputs={len(dit_outputs)} "
              f"(æœŸæœ› ~ {args.steps * approx_calls_per_step})")

    print("\nğŸ‰ å®Œæˆã€‚æ‰€æœ‰è¾“å‡ºä½äºï¼š", session_dir)

if __name__ == "__main__":
    main()

