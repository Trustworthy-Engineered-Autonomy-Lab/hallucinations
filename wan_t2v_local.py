#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Wan2.1 T2V-1.3B (Diffusers) â€”â€” æœ¬åœ°ä¸‹è½½æƒé‡ + å•å¡æ¨ç†ç”Ÿæˆè§†é¢‘è„šæœ¬
- é»˜è®¤ä¸‹è½½åˆ° ./Wan2.1-T2V-1.3B-Diffusers
- é»˜è®¤åˆ†è¾¨ç‡ 832x480ï¼Œå¸§æ•° 81ï¼ŒFPS=15
- è‡ªåŠ¨é€‰æ‹© torch_dtypeï¼ˆä¼˜å…ˆ bfloat16ï¼Œå…¶æ¬¡ float16ï¼›CPU å›é€€ float32ï¼‰
- å¯é€‰ CPU-Offload ä¸ T5 ä»…åœ¨ CPU ä»¥èŠ‚çœæ˜¾å­˜
ç”¨æ³•ç¤ºä¾‹ï¼š
  python wan_t2v_local.py --prompt "A cat walks on the grass, realistic" --offload --t5_cpu
"""

import os
import sys
import argparse
from pathlib import Path

# ç¦ç”¨ xformersï¼ˆé¿å…ä¸ä½ æœºå™¨ä¸Šçš„ flash-attn/xformers å¼•å‘å¯¼å…¥å†²çªï¼‰
os.environ.setdefault("XFORMERS_DISABLED", "1")

def parse_args():
    p = argparse.ArgumentParser(description="Wan2.1 T2V-1.3B (Diffusers) local inference")
    p.add_argument("--model_id", default="Wan-AI/Wan2.1-T2V-1.3B-Diffusers",
                   help="Hugging Face repo id")
    p.add_argument("--local_dir", default="./Wan2.1-T2V-1.3B-Diffusers",
                   help="æœ¬åœ°æƒé‡ä¿å­˜/è¯»å–ç›®å½•")
    p.add_argument("--prompt", default="A cat walks on the grass, realistic",
                   help="æ­£å‘æç¤ºè¯")
    p.add_argument("--negative_prompt", default=("Bright tones, overexposed, static, blurred details, subtitles, "
                                                 "style, works, paintings, images, static, overall gray, worst quality, "
                                                 "low quality, JPEG compression residue, ugly, incomplete, extra fingers, "
                                                 "poorly drawn hands, poorly drawn faces, deformed, disfigured, "
                                                 "misshapen limbs, fused fingers, still picture, messy background, "
                                                 "three legs, many people in the background, walking backwards"),
                   help="åå‘æç¤ºè¯")
    p.add_argument("--width", type=int, default=832, help="ç”Ÿæˆå®½åº¦ï¼ˆéœ€ä¸º32çš„å€æ•°ï¼‰")
    p.add_argument("--height", type=int, default=480, help="ç”Ÿæˆé«˜åº¦ï¼ˆéœ€ä¸º32çš„å€æ•°ï¼‰")
    p.add_argument("--num_frames", type=int, default=81, help="è§†é¢‘å¸§æ•°ï¼ˆä¾‹å¦‚81å¯¹åº”çº¦5.4ç§’@15fpsï¼‰")
    p.add_argument("--fps", type=int, default=15, help="å¯¼å‡ºè§†é¢‘å¸§ç‡")
    p.add_argument("--guidance_scale", type=float, default=5.0, help="CFG å¼•å¯¼ç³»æ•°ï¼ˆå®˜æ–¹ç¤ºä¾‹ 5~6ï¼‰")
    p.add_argument("--out", default="output.mp4", help="è¾“å‡ºè§†é¢‘æ–‡ä»¶å")
    p.add_argument("--device", default="cuda", choices=["cuda", "cpu"], help="æ¨ç†è®¾å¤‡")
    p.add_argument("--offload", action="store_true", help="å¯ç”¨æ¨¡å‹ CPU-Offload ä»¥çœæ˜¾å­˜")
    p.add_argument("--t5_cpu", action="store_true", help="å°† T5 æ–‡æœ¬ç¼–ç å™¨å›ºå®šåœ¨ CPU ä»¥çœæ˜¾å­˜")
    p.add_argument("--seed", type=int, default=None, help="éšæœºç§å­ï¼ˆå¯å¤ç°ï¼‰")
    return p.parse_args()

def main():
    args = parse_args()

    # åŸºç¡€æ£€æŸ¥
    try:
        import torch
    except Exception:
        print("âŒ éœ€è¦å®‰è£… PyTorchï¼ˆå»ºè®® torch>=2.4.0ï¼‰ï¼špip install torch --index-url https://download.pytorch.org/whl/cu121", file=sys.stderr)
        raise

    if args.device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸ æœªæ£€æµ‹åˆ°å¯ç”¨ GPUï¼Œå°†æ”¹ç”¨ CPUï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
        args.device = "cpu"

    # é€‰æ‹© dtypeï¼šä¼˜å…ˆ bfloat16ï¼ˆAmpere+ï¼‰ï¼Œå¦åˆ™ float16ï¼›CPU å›é€€ float32
    if args.device == "cuda":
        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    else:
        dtype = torch.float32

    # ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
    local_dir = Path(args.local_dir).expanduser().resolve()
    if not local_dir.exists() or not any(local_dir.iterdir()):
        print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½æƒé‡åˆ°æœ¬åœ°ï¼š{local_dir}")
        try:
            from huggingface_hub import snapshot_download
        except Exception:
            print("â¡ï¸ å®‰è£… huggingface_hubï¼š pip install -U 'huggingface_hub[cli]'", file=sys.stderr)
            raise
        snapshot_download(
            repo_id=args.model_id,
            local_dir=str(local_dir),
            local_dir_use_symlinks=False,   # ç›´æ¥è½åœ°å®Œæ•´æ–‡ä»¶ï¼Œä¾¿äºç¦»çº¿
            ignore_patterns=["*.msgpack", "*.h5", "*.onnx"],  # å¯é€‰ï¼šç•¥è¿‡ä¸å¿…è¦æ–‡ä»¶
        )
    else:
        print(f"âœ… å·²æ£€æµ‹åˆ°æœ¬åœ°æƒé‡ç›®å½•ï¼š{local_dir}")

    # è½½å…¥ Diffusers ç®¡çº¿
    print("ğŸš€ æ­£åœ¨åŠ è½½ Diffusers ç®¡çº¿ â€¦")
    from diffusers import AutoencoderKLWan, WanPipeline
    from diffusers.utils import export_to_video

    # å•ç‹¬åŠ è½½ VAE å­ç›®å½•ï¼ˆå®˜æ–¹ç¤ºä¾‹ï¼‰
    vae = AutoencoderKLWan.from_pretrained(
        str(local_dir),
        subfolder="vae",
        torch_dtype=(torch.float32 if args.device == "cpu" else torch.float16)
    )

    pipe = WanPipeline.from_pretrained(
        str(local_dir),
        vae=vae,
        torch_dtype=dtype
    )

    # æ”¾åˆ°è®¾å¤‡
    pipe.to(args.device)

    # çœæ˜¾å­˜é€‰é¡¹
    if args.offload:
        # ä¸¤ç§ offload æ–¹æ¡ˆï¼šæŒ‰éœ€ç”Ÿæ•ˆ
        try:
            pipe.enable_sequential_cpu_offload()
        except Exception:
            pass
        try:
            pipe.enable_model_cpu_offload()
        except Exception:
            pass

    # T5 æ–‡æœ¬ç¼–ç å™¨æ”¾ CPUï¼ˆè¿›ä¸€æ­¥çœæ˜¾å­˜ï¼›å• 4090 å¸¸ç”¨ï¼‰
    if args.t5_cpu:
        try:
            pipe.text_encoder.to("cpu")
        except Exception:
            print("â„¹ï¸ å½“å‰ç®¡çº¿ä¸å« text_encoder æˆ–å·²åœ¨ CPUã€‚")

    # è®¾ç½®éšæœºç§å­ï¼ˆå¯å¤ç°ï¼‰
    if args.seed is not None:
        import random
        import numpy as np
        torch.manual_seed(args.seed)
        random.seed(args.seed)
        np.random.seed(args.seed)

    # å‚æ•°æ ¡éªŒ
    if args.width % 32 != 0 or args.height % 32 != 0:
        raise ValueError("width/height å¿…é¡»ä¸º 32 çš„å€æ•°ï¼ˆä¾‹å¦‚ 832x480ï¼‰ã€‚")

    # ç”Ÿæˆ
    print("ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘ â€¦ï¼ˆé¦–æ¬¡è¿è¡Œä¼šæ…¢ä¸€äº›ï¼‰")
    output_frames = pipe(
        prompt=args.prompt,
        negative_prompt=args.negative_prompt,
        height=args.height,
        width=args.width,
        num_frames=args.num_frames,
        guidance_scale=args.guidance_scale,
    ).frames[0]

    # å¯¼å‡ºè§†é¢‘
    out_path = Path(args.out).resolve()
    export_to_video(output_frames, str(out_path), fps=args.fps)
    print(f"âœ… å·²ä¿å­˜è§†é¢‘åˆ°ï¼š{out_path}")

if __name__ == "__main__":
    main()

