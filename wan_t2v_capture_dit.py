#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Wan2.1 T2V-1.3B (Diffusers) â€”â€” å°åˆ†è¾¨ç‡/çŸ­è§†é¢‘ + æ•è· DiT è¾“å…¥è¾“å‡º latent åˆ°æœ¬åœ°
ä¿å­˜å†…å®¹ï¼š
  - dit_inputs.pt:  [Tensor, Tensor, ...]  ï¼ˆæŒ‰è°ƒç”¨é¡ºåºï¼‰
  - dit_outputs.pt: [Tensor, Tensor, ...]  ï¼ˆæŒ‰è°ƒç”¨é¡ºåºï¼‰
  - step_map.json:  { "calls": [ {"call_idx": i, "step": s, "is_cfg_negative": 0/1}, ... ] }
  - metadata.json:  è¿è¡Œå‚æ•°ã€å½¢çŠ¶ã€dtypeã€è®¾å¤‡ç­‰
  - output.mp4:     ç”Ÿæˆè§†é¢‘

å»ºè®®åœ¨è¿è¡Œå‰å¸è½½ xformers/flash-attnï¼ˆé¿å…ä½ ä¹‹å‰ç¢°åˆ°çš„å¯¼å…¥å†²çªï¼‰ï¼š
  pip uninstall -y xformers flash-attn flash_attn flash_attn_2
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path

# é¿å… xformers å¸¦æ¥çš„å¯¼å…¥é—®é¢˜ï¼ˆå¯é€‰ï¼‰
os.environ.setdefault("XFORMERS_DISABLED", "1")

def parse_args():
    p = argparse.ArgumentParser(description="Wan2.1 T2V-1.3B: capture DiT latents")
    p.add_argument("--model_id", default="Wan-AI/Wan2.1-T2V-1.3B-Diffusers", help="HF repo id")
    p.add_argument("--local_dir", default="./Wan2.1-T2V-1.3B-Diffusers", help="æœ¬åœ°æƒé‡ç›®å½•")
    p.add_argument("--save_dir", default="./wan_dit_dump", help="ä¿å­˜è¾“å‡ºä¸latentçš„ç›®å½•")
    p.add_argument("--prompt", default="A cat walks on the grass, realistic", help="æ­£å‘æç¤ºè¯")
    p.add_argument("--negative_prompt", default=("Bright tones, overexposed, static, blurred details, subtitles, "
                                                 "style, works, paintings, images, static, overall gray, worst quality, "
                                                 "low quality, JPEG compression residue, ugly, incomplete, extra fingers, "
                                                 "poorly drawn hands, poorly drawn faces, deformed, disfigured, "
                                                 "misshapen limbs, fused fingers, still picture, messy background, "
                                                 "three legs, many people in the background, walking backwards"),
                   help="åå‘æç¤ºè¯")
    # å°ä¸€äº›çš„åˆ†è¾¨ç‡ä¸å¸§æ•°
    p.add_argument("--width", type=int, default=448, help="å®½ï¼ˆéœ€ä¸º32çš„å€æ•°ï¼‰")
    p.add_argument("--height", type=int, default=256, help="é«˜ï¼ˆéœ€ä¸º32çš„å€æ•°ï¼‰")
    p.add_argument("--num_frames", type=int, default=33, help="å¸§æ•°ï¼ˆè¶Šå°è¶Šçœèµ„æºï¼‰")
    p.add_argument("--fps", type=int, default=15, help="è§†é¢‘å¸§ç‡")
    p.add_argument("--guidance_scale", type=float, default=5.0, help="CFG å¼•å¯¼ç³»æ•°")
    p.add_argument("--steps", type=int, default=20, help="å»å™ªæ­¥æ•°ï¼ˆè¶Šå°è¶Šå¿«ï¼‰")
    p.add_argument("--device", default="cuda", choices=["cuda", "cpu"], help="æ¨ç†è®¾å¤‡")
    p.add_argument("--offload", action="store_true", help="å¯ç”¨ CPU-Offload çœæ˜¾å­˜")
    p.add_argument("--t5_cpu", action="store_true", help="T5 æ–‡æœ¬ç¼–ç å™¨æ”¾åˆ° CPU")
    p.add_argument("--seed", type=int, default=None, help="éšæœºç§å­")
    return p.parse_args()

def ensure_model(local_dir_str, repo_id):
    local_dir = Path(local_dir_str).expanduser().resolve()
    if not local_dir.exists() or not any(local_dir.iterdir()):
        print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½æƒé‡åˆ°ï¼š{local_dir}")
        try:
            from huggingface_hub import snapshot_download
        except Exception:
            print("â¡ï¸ è¯·å…ˆå®‰è£… huggingface_hubï¼špip install -U 'huggingface_hub[cli]'", file=sys.stderr)
            raise
        snapshot_download(
            repo_id=repo_id,
            local_dir=str(local_dir),
            local_dir_use_symlinks=False,
        )
    else:
        print(f"âœ… ä½¿ç”¨æœ¬åœ°æƒé‡ç›®å½•ï¼š{local_dir}")
    return local_dir

def main():
    args = parse_args()
    import torch

    if args.device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° CPUï¼ˆä¼šå¾ˆæ…¢ï¼‰")
        args.device = "cpu"

    # dtype é€‰æ‹©ï¼šGPUä¼˜å…ˆ bf16 -> fp16ï¼ŒCPU ç”¨ fp32
    if args.device == "cuda":
        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    else:
        dtype = torch.float32

    # åŸºæœ¬æ ¡éªŒ
    if args.width % 32 != 0 or args.height % 32 != 0:
        raise ValueError("width/height å¿…é¡»ä¸º 32 çš„å€æ•°ï¼Œä¾‹å¦‚ 448x256")

    save_root = Path(args.save_dir).expanduser().resolve()
    ts_tag = time.strftime("%Y%m%d-%H%M%S")
    run_dir = save_root / f"run-{ts_tag}"
    run_dir.mkdir(parents=True, exist_ok=True)

    # ä¸‹è½½æˆ–ä½¿ç”¨æœ¬åœ°æƒé‡
    local_dir = ensure_model(args.local_dir, args.model_id)

    # åŠ è½½ç®¡çº¿
    print("ğŸš€ åŠ è½½ Diffusers WanPipeline â€¦")
    from diffusers import AutoencoderKLWan, WanPipeline
    from diffusers.utils import export_to_video

    vae = AutoencoderKLWan.from_pretrained(
        str(local_dir),
        subfolder="vae",
        torch_dtype=(torch.float32 if args.device == "cpu" else torch.float16)
    )
    pipe = WanPipeline.from_pretrained(
        str(local_dir),
        vae=vae,
        torch_dtype=dtype,
    ).to(args.device)

    # çœæ˜¾å­˜
    if args.offload:
        try: pipe.enable_sequential_cpu_offload()
        except Exception: pass
        try: pipe.enable_model_cpu_offload()
        except Exception: pass

    if args.t5_cpu:
        try:
            pipe.text_encoder.to("cpu")
        except Exception:
            print("â„¹ï¸ å½“å‰ç®¡çº¿ä¸å« text_encoder æˆ–å·²åœ¨ CPUã€‚")

    # æ‰¾åˆ° DiT / transformer æ¨¡å—ï¼ˆWanPipeline é»˜è®¤åº”ä¸º pipe.transformerï¼‰
    if not hasattr(pipe, "transformer"):
        raise RuntimeError("æœªåœ¨ WanPipeline ä¸­æ‰¾åˆ° transformerï¼ˆDiTï¼‰æ¨¡å—ï¼Œæ— æ³•æŒ‚é’©æ•è· latentã€‚")

    transformer = pipe.transformer

    # æ•è·å®¹å™¨
    dit_inputs, dit_outputs = [], []
    call_records = []  # è®°å½•æ¯æ¬¡è°ƒç”¨å¤§è‡´å¯¹åº”çš„ step ä¸æ˜¯å¦è´Ÿåˆ†æ”¯ï¼ˆCFGï¼‰
    # æ³¨ï¼šä¸¥æ ¼çš„â€œæ­¥å·/æ­£è´Ÿåˆ†æ”¯â€åœ¨ä¸æ”¹ pipeline å†…éƒ¨ä»£ç æ—¶éš¾ä»¥ç²¾å‡†æ‹¿åˆ°ï¼ˆDiffuserså†…éƒ¨ä¼šæŠŠæ¡ä»¶/æ— æ¡ä»¶æ‹¼æ¥/æ‹†åˆ†ï¼‰ã€‚
    # è¿™é‡Œé‡‡ç”¨å¯å‘å¼ï¼šæŒ‰è°ƒç”¨é¡ºåºè®°å½•ï¼Œå¹¶åœ¨ metadata é‡Œä¿å­˜ num_inference_steps ä¸ guidance_scaleï¼Œä¾¿äºä½ åå¤„ç†æ˜ å°„ã€‚

    def _to_cpu_detached(x):
        if isinstance(x, torch.Tensor):
            return x.detach().to("cpu")
        if isinstance(x, (list, tuple)):
            return type(x)(_to_cpu_detached(xx) for xx in x)
        if isinstance(x, dict):
            return {k: _to_cpu_detached(v) for k, v in x.items()}
        return x

    def pre_hook(mod, inputs):
        # inputs æ˜¯ä¸€ä¸ª tupleï¼›é€šå¸¸ç¬¬ä¸€ä¸ªæ˜¯å™ªå£° latentï¼ˆæˆ–å…¶å˜ä½“ï¼‰
        dit_inputs.append(_to_cpu_detached(inputs))
        # è®°å½•åŸºæœ¬ä¿¡æ¯ï¼ˆæ­¥å·æ¨æ–­äº¤ç»™åå¤„ç†ï¼Œè¿™é‡Œåªå ä½ï¼‰
        call_records.append({"call_idx": len(dit_inputs)-1})

    def fwd_hook(mod, inputs, output):
        # output å¯èƒ½æ˜¯å¼ é‡æˆ– tuple
        if isinstance(output, (list, tuple)):
            out = output[0]
        else:
            out = output
        dit_outputs.append(_to_cpu_detached(out))

    # æ³¨å†Œ hook
    pre_h = transformer.register_forward_pre_hook(pre_hook, with_kwargs=False)
    fwd_h = transformer.register_forward_hook(fwd_hook, with_kwargs=False)

    # éšæœºç§å­
    if args.seed is not None:
        import random, numpy as np
        torch.manual_seed(args.seed); random.seed(args.seed); np.random.seed(args.seed)

    print("ğŸ¬ å¼€å§‹ç”Ÿæˆï¼ˆå°åˆ†è¾¨ç‡+çŸ­è§†é¢‘ï¼‰å¹¶æ•è· DiT latent â€¦")
    try:
        result = pipe(
            prompt=args.prompt,
            negative_prompt=args.negative_prompt,
            height=args.height,
            width=args.width,
            num_frames=args.num_frames,
            guidance_scale=args.guidance_scale,
            num_inference_steps=args.steps,   # æ§åˆ¶æ­¥æ•°ï¼ˆDiffusersæ”¯æŒï¼‰
        )
    finally:
        # ç¡®ä¿ç§»é™¤ hookï¼Œé¿å…æ½œåœ¨çš„å†…å­˜/é‡å¤æ³¨å†Œé—®é¢˜
        try: pre_h.remove()
        except Exception: pass
        try: fwd_h.remove()
        except Exception: pass

    frames = result.frames[0]

    # ä¿å­˜è§†é¢‘
    out_video = run_dir / "output.mp4"
    export_to_video(frames, str(out_video), fps=args.fps)
    print(f"âœ… å·²ä¿å­˜è§†é¢‘ï¼š{out_video}")

    # ä¿å­˜ latentï¼ˆç»Ÿä¸€ç”¨ torch.saveï¼‰
    import torch as _torch
    _torch.save(dit_inputs, run_dir / "dit_inputs.pt")
    _torch.save(dit_outputs, run_dir / "dit_outputs.pt")

    # å…ƒä¿¡æ¯
    meta = {
        "prompt": args.prompt,
        "negative_prompt": args.negative_prompt,
        "width": args.width,
        "height": args.height,
        "num_frames": args.num_frames,
        "fps": args.fps,
        "guidance_scale": args.guidance_scale,
        "num_inference_steps": args.steps,
        "dtype": str(dtype),
        "device": args.device,
        "model_dir": str(local_dir),
        "save_dir": str(run_dir),
        "seed": args.seed,
        "dit_calls": len(dit_inputs),
        "notes": "DiT è¾“å…¥/è¾“å‡ºæ˜¯æŒ‰ forward è°ƒç”¨é¡ºåºä¿å­˜çš„ï¼›è‹¥ä½¿ç”¨äº† CFGï¼Œæ¯ä¸€æ­¥é€šå¸¸ä¼šæœ‰ä¸¤æ¬¡è°ƒç”¨ï¼ˆæ— æ¡ä»¶/æœ‰æ¡ä»¶ï¼‰ã€‚",
    }
    with open(run_dir / "metadata.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    # ç®€å•çš„è°ƒç”¨åˆ°æ­¥æ•°çš„å¯å‘å¼æ˜ å°„ï¼ˆä¸ä¿è¯ä¸¥æ ¼å‡†ç¡®ï¼Œä½†ä¾¿äºåå¤„ç†ï¼‰
    # å¸¸è§æƒ…å†µä¸‹ï¼šæ¯ä¸ªå»å™ªæ­¥ stepï¼Œä¼šæœ‰ 1ï¼ˆæ— CFGï¼‰æˆ– 2ï¼ˆæœ‰CFGï¼‰æ¬¡ DiT è°ƒç”¨
    approx_calls_per_step = 2 if args.guidance_scale and args.guidance_scale > 1.0 else 1
    call_map = []
    for i in range(len(dit_inputs)):
        step_idx = i // approx_calls_per_step
        is_cfg_negative = (i % approx_calls_per_step == 0 and approx_calls_per_step == 2)  # ç²—ç•¥æ ‡æ³¨
        call_map.append({"call_idx": i, "approx_step": int(step_idx), "is_cfg_negative": int(is_cfg_negative)})
    with open(run_dir / "step_map.json", "w", encoding="utf-8") as f:
        json.dump({"calls": call_map}, f, ensure_ascii=False, indent=2)

    print(f"âœ… å·²ä¿å­˜ DiT latent ä¸å…ƒä¿¡æ¯åˆ°ç›®å½•ï¼š{run_dir}")
    print("   - dit_inputs.pt / dit_outputs.pt / metadata.json / step_map.json / output.mp4")
    print("â„¹ï¸ å¦‚éœ€ä¸¥æ ¼å¯¹é½æ¯æ¬¡è°ƒç”¨ä¸å…·ä½“ scheduler æ­¥ï¼Œè¯·è€ƒè™‘ä¿®æ”¹ Diffusers æºç æˆ–åœ¨ pipeline å†…éƒ¨ callback å¤„æ‰“ç‚¹ã€‚")

if __name__ == "__main__":
    main()

