#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Wan2.1 T2V-1.3B (Diffusers)
- åˆ†è¾¨ç‡: 832x480
- å¸§æ•°:   81
- ä¿å­˜ DiT è¾“å…¥/è¾“å‡º latentï¼ˆæŒ‰ forward è°ƒç”¨é¡ºåºï¼‰
- è‡ªåŠ¨æœ¬åœ°ç¼“å­˜æƒé‡
- å¯é€‰ CPU-Offload ä¸ T5 ä»…åœ¨ CPU ä»¥çœæ˜¾å­˜

ç”¨æ³•ç¤ºä¾‹ï¼š
  python wan_t2v_capture_dit_full.py --offload --t5_cpu \
    --prompt "A cat walks on the grass, realistic"

è¾“å‡ºç›®å½•ç»“æ„ï¼š
  wan_dit_dump/run-YYYYMMDD-HHMMSS/
    â”œâ”€â”€ output.mp4
    â”œâ”€â”€ dit_inputs.pt      # list of tuples (each is the forward inputs)
    â”œâ”€â”€ dit_outputs.pt     # list of Tensors (forward output)
    â”œâ”€â”€ metadata.json
    â””â”€â”€ step_map.json      # æŒ‰è°ƒç”¨é¡ºåºçš„å¯å‘å¼æ­¥æ•°æ˜ å°„ï¼ˆCFG ç²—ç•¥æ ‡æ³¨ï¼‰
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path

# é¿å… xformers/flash-attn å¯¼å…¥å†²çªï¼ˆå¯é€‰ï¼‰
os.environ.setdefault("XFORMERS_DISABLED", "1")

def parse_args():
    p = argparse.ArgumentParser(description="Wan2.1 T2V-1.3B: full video + capture DiT latents")
    p.add_argument("--model_id", default="Wan-AI/Wan2.1-T2V-1.3B-Diffusers", help="HF repo id")
    p.add_argument("--local_dir", default="./Wan2.1-T2V-1.3B-Diffusers", help="æœ¬åœ°æƒé‡ç›®å½•")

    # æ­£å¸¸é•¿åº¦ä¸å°ºå¯¸
    p.add_argument("--width", type=int, default=832, help="å®½ï¼ˆéœ€ä¸º32çš„å€æ•°ï¼‰")
    p.add_argument("--height", type=int, default=480, help="é«˜ï¼ˆéœ€ä¸º32çš„å€æ•°ï¼‰")
    p.add_argument("--num_frames", type=int, default=81, help="å¸§æ•°ï¼ˆé»˜è®¤ 81 â‰ˆ 5.4s @15fpsï¼‰")
    p.add_argument("--fps", type=int, default=15, help="å¯¼å‡ºè§†é¢‘å¸§ç‡")

    # é‡‡æ ·/CFG
    p.add_argument("--guidance_scale", type=float, default=5.5, help="CFG å¼•å¯¼ç³»æ•°ï¼ˆæ¨è 5~6ï¼‰")
    p.add_argument("--steps", type=int, default=30, help="å»å™ªæ­¥æ•°ï¼ˆDiffusers çš„ num_inference_stepsï¼‰")

    # æç¤ºè¯
    p.add_argument("--prompt", default="A cat walks on the grass, realistic", help="æ­£å‘æç¤ºè¯")
    p.add_argument("--negative_prompt", default=("Bright tones, overexposed, static, blurred details, subtitles, "
                                                 "style, works, paintings, images, static, overall gray, worst quality, "
                                                 "low quality, JPEG compression residue, ugly, incomplete, extra fingers, "
                                                 "poorly drawn hands, poorly drawn faces, deformed, disfigured, "
                                                 "misshapen limbs, fused fingers, still picture, messy background, "
                                                 "three legs, many people in the background, walking backwards"),
                   help="åå‘æç¤ºè¯")

    # è®¾å¤‡/å†…å­˜
    p.add_argument("--device", default="cuda", choices=["cuda", "cpu"], help="æ¨ç†è®¾å¤‡")
    p.add_argument("--offload", action="store_true", help="å¯ç”¨ CPU-Offload çœæ˜¾å­˜")
    p.add_argument("--t5_cpu", action="store_true", help="T5 æ–‡æœ¬ç¼–ç å™¨å›ºå®šåœ¨ CPU")
    p.add_argument("--seed", type=int, default=None, help="éšæœºç§å­ï¼ˆå¯å¤ç°ï¼‰")

    p.add_argument("--save_root", default="./wan_dit_dump", help="ä¿å­˜è¾“å‡ºä¸ latent çš„æ ¹ç›®å½•")
    return p.parse_args()

def ensure_model(local_dir_str, repo_id):
    local_dir = Path(local_dir_str).expanduser().resolve()
    if not local_dir.exists() or not any(local_dir.iterdir()):
        print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½æƒé‡åˆ°ï¼š{local_dir}")
        try:
            from huggingface_hub import snapshot_download
        except Exception:
            print("â¡ï¸ è¯·å…ˆå®‰è£… huggingface_hubï¼špip install -U 'huggingface_hub[cli]'", file=sys.stderr)
            raise
        snapshot_download(
            repo_id=repo_id,
            local_dir=str(local_dir),
            local_dir_use_symlinks=False,
        )
    else:
        print(f"âœ… ä½¿ç”¨æœ¬åœ°æƒé‡ç›®å½•ï¼š{local_dir}")
    return local_dir

def main():
    args = parse_args()

    # PyTorch & è®¾å¤‡
    import torch
    if args.device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° CPUï¼ˆä¼šå¾ˆæ…¢ï¼‰")
        args.device = "cpu"

    # dtypeï¼šGPU ä¼˜å…ˆ bfloat16 -> float16ï¼ŒCPU ç”¨ float32
    if args.device == "cuda":
        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    else:
        dtype = torch.float32

    # åŸºæœ¬æ ¡éªŒ
    if args.width % 32 != 0 or args.height % 32 != 0:
        raise ValueError("width/height å¿…é¡»ä¸º 32 çš„å€æ•°ï¼Œä¾‹å¦‚ 832x480")

    # è¿è¡Œç›®å½•
    save_root = Path(args.save_root).expanduser().resolve()
    ts_tag = time.strftime("%Y%m%d-%H%M%S")
    run_dir = save_root / f"run-{ts_tag}"
    run_dir.mkdir(parents=True, exist_ok=True)

    # æƒé‡
    local_dir = ensure_model(args.local_dir, args.model_id)

    # åŠ è½½ç®¡çº¿
    print("ğŸš€ åŠ è½½ Diffusers WanPipeline â€¦")
    from diffusers import AutoencoderKLWan, WanPipeline
    from diffusers.utils import export_to_video

    vae = AutoencoderKLWan.from_pretrained(
        str(local_dir),
        subfolder="vae",
        torch_dtype=(torch.float32 if args.device == "cpu" else torch.float16)
    )
    pipe = WanPipeline.from_pretrained(
        str(local_dir),
        vae=vae,
        torch_dtype=dtype
    ).to(args.device)

    # çœæ˜¾å­˜
    if args.offload:
        try: pipe.enable_sequential_cpu_offload()
        except Exception: pass
        try: pipe.enable_model_cpu_offload()
        except Exception: pass

    if args.t5_cpu:
        try:
            pipe.text_encoder.to("cpu")
        except Exception:
            print("â„¹ï¸ å½“å‰ç®¡çº¿ä¸å« text_encoder æˆ–å·²åœ¨ CPUã€‚")

    # éšæœºç§å­
    if args.seed is not None:
        import random, numpy as np
        torch.manual_seed(args.seed); random.seed(args.seed); np.random.seed(args.seed)

    # æŠ“å– DiTï¼ˆtransformerï¼‰latent
    if not hasattr(pipe, "transformer"):
        raise RuntimeError("æœªåœ¨ WanPipeline ä¸­æ‰¾åˆ° transformerï¼ˆDiTï¼‰æ¨¡å—ï¼Œæ— æ³•æŒ‚é’©æ•è· latentã€‚")
    transformer = pipe.transformer

    dit_inputs, dit_outputs = [], []

    def _to_cpu_detached(x):
        if isinstance(x, torch.Tensor):
            return x.detach().to("cpu")
        if isinstance(x, (list, tuple)):
            return type(x)(_to_cpu_detached(xx) for xx in x)
        if isinstance(x, dict):
            return {k: _to_cpu_detached(v) for k, v in x.items()}
        return x

    def pre_hook(mod, inputs):
        # inputs æ˜¯ tupleï¼›é€šå¸¸ç¬¬ä¸€ä¸ªæ˜¯ latent
        dit_inputs.append(_to_cpu_detached(inputs))

    def fwd_hook(mod, inputs, output):
        # output å¯èƒ½æ˜¯ tensor æˆ– tuple
        if isinstance(output, (list, tuple)):
            out = output[0]
        else:
            out = output
        dit_outputs.append(_to_cpu_detached(out))

    pre_h = transformer.register_forward_pre_hook(pre_hook, with_kwargs=False)
    fwd_h = transformer.register_forward_hook(fwd_hook, with_kwargs=False)

    print("ğŸ¬ å¼€å§‹ç”Ÿæˆ 832x480, 81 å¸§ çš„è§†é¢‘ï¼Œå¹¶æ•è· DiT latent â€¦")
    try:
        result = pipe(
            prompt=args.prompt,
            negative_prompt=args.negative_prompt,
            height=args.height,
            width=args.width,
            num_frames=args.num_frames,
            guidance_scale=args.guidance_scale,
            num_inference_steps=args.steps,
        )
    finally:
        # ç§»é™¤ hookï¼Œé¿å…é‡å¤æ³¨å†Œä¸æ˜¾å­˜æ³„æ¼
        try: pre_h.remove()
        except Exception: pass
        try: fwd_h.remove()
        except Exception: pass

    frames = result.frames[0]

    # å¯¼å‡ºè§†é¢‘
    out_video = run_dir / "output.mp4"
    export_to_video(frames, str(out_video), fps=args.fps)
    print(f"âœ… å·²ä¿å­˜è§†é¢‘ï¼š{out_video}")

    # ä¿å­˜ latent
    import torch as _torch
    _torch.save(dit_inputs,  run_dir / "dit_inputs.pt")
    _torch.save(dit_outputs, run_dir / "dit_outputs.pt")

    # å…ƒä¿¡æ¯
    meta = {
        "prompt": args.prompt,
        "negative_prompt": args.negative_prompt,
        "width": args.width,
        "height": args.height,
        "num_frames": args.num_frames,
        "fps": args.fps,
        "guidance_scale": args.guidance_scale,
        "num_inference_steps": args.steps,
        "dtype": str(dtype),
        "device": args.device,
        "model_dir": str(local_dir),
        "save_dir": str(run_dir),
        "seed": args.seed,
        "dit_calls": len(dit_inputs),
        "notes": "DiT è¾“å…¥/è¾“å‡ºæŒ‰ forward è°ƒç”¨é¡ºåºä¿å­˜ï¼›ä½¿ç”¨ CFG æ—¶é€šå¸¸æ¯æ­¥ä¸¤æ¬¡ï¼ˆæ— æ¡ä»¶/æœ‰æ¡ä»¶ï¼‰ã€‚",
    }
    with open(run_dir / "metadata.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    # å¯å‘å¼æ­¥æ•°æ˜ å°„ï¼ˆä¸ä¿è¯ä¸¥æ ¼å‡†ç¡®ï¼›ä¾¿äºåå¤„ç†ï¼‰
    approx_calls_per_step = 2 if args.guidance_scale and args.guidance_scale > 1.0 else 1
    call_map = []
    for i in range(len(dit_inputs)):
        step_idx = i // approx_calls_per_step
        is_cfg_negative = (i % approx_calls_per_step == 0 and approx_calls_per_step == 2)
        call_map.append({"call_idx": i, "approx_step": int(step_idx), "is_cfg_negative": int(is_cfg_negative)})
    with open(run_dir / "step_map.json", "w", encoding="utf-8") as f:
        json.dump({"calls": call_map}, f, ensure_ascii=False, indent=2)

    print(f"âœ… å·²ä¿å­˜ DiT latent ä¸å…ƒä¿¡æ¯åˆ°ï¼š{run_dir}")
    print("   - dit_inputs.pt / dit_outputs.pt / metadata.json / step_map.json / output.mp4")

if __name__ == "__main__":
    main()

