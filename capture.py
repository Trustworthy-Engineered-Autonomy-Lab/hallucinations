#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Wan2.1 T2V-1.3B (Diffusers) â€”â€” æ•è· DiT è¾“å…¥/è¾“å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰
- ä¿®å¤ï¼špre-hook æ•è·åˆ°çš„ inputs ä¸ºç©º () çš„é—®é¢˜ï¼›ç°åœ¨ä¼šæŠŠæ‰€æœ‰ Tensor è½¬æˆ cpu + detach åä¿å­˜
- åŒæ—¶æ•è· kwargsï¼ˆä»¥ JSON æ‘˜è¦å½¢å¼ä¿å­˜å…¶å¼ é‡å½¢çŠ¶ï¼Œé¿å…é‡å¤ä¿å­˜å¤§å¼ é‡ï¼‰
- æ­£å¸¸åˆ†è¾¨ç‡ä¸æ—¶é•¿ï¼š832x480, 81 å¸§ï¼›å¯é€šè¿‡å‚æ•°ä¿®æ”¹
"""

import os
import sys
import json
import time
from pathlib import Path
import argparse

os.environ.setdefault("XFORMERS_DISABLED", "1")  # è§„é¿ xformers/flash-attn å¯¼å…¥å†²çª

def parse_args():
    p = argparse.ArgumentParser(description="Wan2.1 T2V-1.3B capture DiT inputs/outputs (fixed)")
    p.add_argument("--model_id", default="Wan-AI/Wan2.1-T2V-1.3B-Diffusers", help="HF repo id")
    p.add_argument("--local_dir", default="./Wan2.1-T2V-1.3B-Diffusers", help="æœ¬åœ°æƒé‡ç›®å½•")
    p.add_argument("--save_root", default="./wan_dit_dump", help="è¾“å‡ºæ ¹ç›®å½•")

    # æ­£å¸¸é•¿åº¦ä¸å°ºå¯¸
    p.add_argument("--width", type=int, default=832)
    p.add_argument("--height", type=int, default=480)
    p.add_argument("--num_frames", type=int, default=81)
    p.add_argument("--fps", type=int, default=15)

    # é‡‡æ ·/CFG
    p.add_argument("--guidance_scale", type=float, default=5.5)
    p.add_argument("--steps", type=int, default=30)

    # æç¤ºè¯
    p.add_argument("--prompt", default="A cat walks on the grass, realistic")
    p.add_argument("--negative_prompt", default=("Bright tones, overexposed, static, blurred details, subtitles, "
                                                 "style, works, paintings, images, static, overall gray, worst quality, "
                                                 "low quality, JPEG compression residue, ugly, incomplete, extra fingers, "
                                                 "poorly drawn hands, poorly drawn faces, deformed, disfigured, "
                                                 "misshapen limbs, fused fingers, still picture, messy background, "
                                                 "three legs, many people in the background, walking backwards"))

    # è®¾å¤‡/å†…å­˜
    p.add_argument("--device", default="cuda", choices=["cuda", "cpu"])
    p.add_argument("--offload", action="store_true", help="å¯ç”¨ CPU-Offload çœæ˜¾å­˜")
    p.add_argument("--t5_cpu", action="store_true", help="T5 æ–‡æœ¬ç¼–ç å™¨å›ºå®šåœ¨ CPU")
    p.add_argument("--seed", type=int, default=None)
    return p.parse_args()

def ensure_model(local_dir_str, repo_id):
    local_dir = Path(local_dir_str).expanduser().resolve()
    if not local_dir.exists() or not any(local_dir.iterdir()):
        print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½æƒé‡åˆ°ï¼š{local_dir}")
        from huggingface_hub import snapshot_download
        snapshot_download(repo_id=repo_id, local_dir=str(local_dir), local_dir_use_symlinks=False)
    else:
        print(f"âœ… ä½¿ç”¨æœ¬åœ°æƒé‡ç›®å½•ï¼š{local_dir}")
    return local_dir

def to_cpu_detached(obj):
    """æŠŠä»»æ„åµŒå¥—ç»“æ„é‡Œçš„ Tensor -> cpu().detach()ï¼›å…¶å®ƒç±»å‹ä¿æŒå¼•ç”¨æˆ–åšè½»é‡è½¬æ¢ã€‚"""
    import torch
    if isinstance(obj, torch.Tensor):
        return obj.detach().to("cpu")
    elif isinstance(obj, (list, tuple)):
        return type(obj)(to_cpu_detached(x) for x in obj)
    elif isinstance(obj, dict):
        return {k: to_cpu_detached(v) for k, v in obj.items()}
    else:
        return obj  # int/float/str/None ç­‰åŸæ ·è¿”å›

def summarize_shapes(obj):
    """ä»…æ‘˜è¦ kwargs ç»“æ„ä¸­çš„å¼ é‡: è¿”å›åŒç»“æ„ã€ä½ç½®ä¸Šç”¨å½¢çŠ¶/ dtype å­—ç¬¦ä¸²ä»£æ›¿å®é™…å¼ é‡ã€‚"""
    import torch
    if isinstance(obj, torch.Tensor):
        return {"_tensor": True, "shape": tuple(obj.shape), "dtype": str(obj.dtype)}
    elif isinstance(obj, (list, tuple)):
        return [summarize_shapes(x) for x in obj]
    elif isinstance(obj, dict):
        return {k: summarize_shapes(v) for k, v in obj.items()}
    else:
        # åŸºæœ¬ç±»å‹ç›´æ¥è¿”å›ï¼›å¤æ‚å¯¹è±¡è¿”å›ç±»å‹å
        if isinstance(obj, (int, float, str, bool)) or obj is None:
            return obj
        return {"_type": type(obj).__name__}

def main():
    args = parse_args()

    # Torch & è®¾å¤‡
    import torch
    if args.device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œæ”¹ç”¨ CPUï¼ˆè¾ƒæ…¢ï¼‰")
        args.device = "cpu"

    dtype = torch.bfloat16 if (args.device == "cuda" and torch.cuda.is_bf16_supported()) else (torch.float16 if args.device == "cuda" else torch.float32)

    # æ ¡éªŒå°ºå¯¸
    if args.width % 32 != 0 or args.height % 32 != 0:
        raise ValueError("width/height å¿…é¡»ä¸º 32 çš„å€æ•°ï¼ˆä¾‹å¦‚ 832x480ï¼‰")

    # ç›®å½•
    save_root = Path(args.save_root).expanduser().resolve()
    run_dir = save_root / f"run-{time.strftime('%Y%m%d-%H%M%S')}"
    run_dir.mkdir(parents=True, exist_ok=True)

    local_dir = ensure_model(args.local_dir, args.model_id)

    print("ğŸš€ åŠ è½½ Diffusers WanPipeline â€¦")
    from diffusers import AutoencoderKLWan, WanPipeline
    from diffusers.utils import export_to_video

    vae = AutoencoderKLWan.from_pretrained(str(local_dir), subfolder="vae",
                                           torch_dtype=(torch.float32 if args.device == "cpu" else torch.float16))
    pipe = WanPipeline.from_pretrained(str(local_dir), vae=vae, torch_dtype=dtype).to(args.device)

    if args.offload:
        try: pipe.enable_sequential_cpu_offload()
        except Exception: pass
        try: pipe.enable_model_cpu_offload()
        except Exception: pass
    if args.t5_cpu:
        try: pipe.text_encoder.to("cpu")
        except Exception: pass

    # éšæœºç§å­
    if args.seed is not None:
        import random, numpy as np
        torch.manual_seed(args.seed); random.seed(args.seed); np.random.seed(args.seed)

    # æ‰¾ transformer
    if not hasattr(pipe, "transformer"):
        raise RuntimeError("æœªæ‰¾åˆ° pipe.transformerï¼ˆDiT æ¨¡å—ï¼‰ã€‚")

    transformer = pipe.transformer

    # å®¹å™¨
    dit_inputs_args = []   # list[tuple(...)]  â€”â€” å·²è½¬æ¢åˆ° cpu
    dit_inputs_kwargs = [] # list[dict ...]    â€”â€” ä»…æ‘˜è¦ç»“æ„å’Œå¼ é‡å½¢çŠ¶
    dit_outputs = []       # list[tensor]      â€”â€” å·²è½¬æ¢åˆ° cpu

    # Hooks
    def pre_hook_args_kwargs(mod, inputs, kwargs=None):
        # inputs: tuple; kwargs: dict or None
        args_cpu = to_cpu_detached(inputs)                    # æŠŠä½ç½®å‚æ•°çš„ Tensor æ‹·åˆ° CPU
        kwargs_summary = summarize_shapes(kwargs or {})       # å…³é”®å­—å‚æ•°åªä¿å­˜å½¢çŠ¶æ‘˜è¦
        dit_inputs_args.append(tuple(args_cpu))
        dit_inputs_kwargs.append(kwargs_summary)

    def fwd_hook(mod, inputs, output):
        # ä»…ä¿å­˜ä»£è¡¨æ€§è¾“å‡ºï¼ˆè‹¥æ˜¯ tuple/list å–ç¬¬ä¸€ä¸ª Tensorï¼‰
        if isinstance(output, (list, tuple)):
            out = output[0]
        else:
            out = output
        if isinstance(out, torch.Tensor):
            dit_outputs.append(out.detach().to("cpu"))
        else:
            # ä¸å¸¸è§åˆ†æ”¯ï¼šéå¼ é‡è¾“å‡ºï¼Œä¿å­˜å ä½ä¿¡æ¯
            import numpy as _np
            dit_outputs.append(torch.tensor(0))  # å ä½ï¼Œé¿å…é•¿åº¦ä¸é½

    # å…¼å®¹ä¸åŒ PyTorch ç‰ˆæœ¬ï¼šä¼˜å…ˆ with_kwargs=Trueï¼Œå¦åˆ™å›é€€
    try:
        pre_h = transformer.register_forward_pre_hook(pre_hook_args_kwargs, with_kwargs=True)
    except TypeError:
        # è€ç‰ˆæœ¬å›é€€ï¼šæ²¡æœ‰ kwargs
        def pre_hook_only_args(mod, inputs):
            pre_hook_args_kwargs(mod, inputs, kwargs=None)
        pre_h = transformer.register_forward_pre_hook(pre_hook_only_args)

    fwd_h = transformer.register_forward_hook(fwd_hook, with_kwargs=False)

    print("ğŸ¬ ç”Ÿæˆè§†é¢‘å¹¶æ•è· DiT inputs/outputs â€¦")
    try:
        result = pipe(
            prompt=args.prompt,
            negative_prompt=args.negative_prompt,
            height=args.height,
            width=args.width,
            num_frames=args.num_frames,
            guidance_scale=args.guidance_scale,
            num_inference_steps=args.steps,
        )
    finally:
        try: pre_h.remove()
        except Exception: pass
        try: fwd_h.remove()
        except Exception: pass

    frames = result.frames[0]
    out_video = run_dir / "output.mp4"
    from diffusers.utils import export_to_video
    export_to_video(frames, str(out_video), fps=args.fps)
    print(f"âœ… å·²ä¿å­˜è§†é¢‘ï¼š{out_video}")

    # ä¿å­˜
    torch.save(dit_inputs_args, run_dir / "dit_inputs.pt")
    torch.save(dit_outputs,    run_dir / "dit_outputs.pt")
    with open(run_dir / "dit_inputs_kwargs.json", "w", encoding="utf-8") as f:
        json.dump(dit_inputs_kwargs, f, ensure_ascii=False, indent=2)

    # å…ƒä¿¡æ¯
    meta = {
        "width": args.width, "height": args.height, "num_frames": args.num_frames, "fps": args.fps,
        "guidance_scale": args.guidance_scale, "num_inference_steps": args.steps,
        "dtype": str(dtype), "device": args.device, "seed": args.seed,
        "model_dir": str(local_dir), "save_dir": str(run_dir)
    }
    with open(run_dir / "metadata.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    # å¯å‘å¼ step mapï¼ˆCFG æƒ…å†µä¸‹æ¯æ­¥ 2 æ¬¡è°ƒç”¨ï¼‰
    approx_calls_per_step = 2 if args.guidance_scale and args.guidance_scale > 1.0 else 1
    call_map = []
    for i in range(len(dit_inputs_args)):
        step_idx = i // approx_calls_per_step
        is_cfg_negative = (i % approx_calls_per_step == 0 and approx_calls_per_step == 2)
        call_map.append({"call_idx": i, "approx_step": int(step_idx), "is_cfg_negative": int(is_cfg_negative)})
    with open(run_dir / "step_map.json", "w", encoding="utf-8") as f:
        json.dump({"calls": call_map}, f, ensure_ascii=False, indent=2)

    print(f"âœ… æ•è·å®Œæˆï¼Œæ–‡ä»¶ä¿å­˜åœ¨ï¼š{run_dir}")
    print("   - dit_inputs.ptï¼ˆä½ç½®å‚æ•°ï¼Œå« latent/timestep/text_embeds ç­‰ï¼ŒTensor å·²è½¬ CPUï¼‰")
    print("   - dit_inputs_kwargs.jsonï¼ˆå…³é”®å­—å‚æ•°çš„ç»“æ„/å½¢çŠ¶æ‘˜è¦ï¼‰")
    print("   - dit_outputs.ptï¼ˆè¾“å‡º Tensorï¼ŒCPUï¼‰")
    print("   - metadata.json / step_map.json / output.mp4")

if __name__ == "__main__":
    main()

